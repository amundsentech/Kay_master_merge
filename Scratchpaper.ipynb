{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for pandas\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cleaningtools as ct \n",
    "import file_config as fconfig\n",
    "import merge_config as config\n",
    "import numpy as np\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_merge(data):\n",
    "    for col in data.columns:\n",
    "        if 'depth_ft'in col.lower():\n",
    "            data['from_ft']=data[col]\n",
    "            data['to_ft']=data['from_ft'].shift(-1)-1\n",
    "            #convert ft to meters\n",
    "            data['from_m']=data['from_ft']* .3281\n",
    "            data['to_m']=data['to_ft'] * .3281\n",
    "            data=data.drop(data.filter(like='Depth').columns,axis=1)\n",
    "        if ('hole' in col.lower())& ('id' in col.lower()):\n",
    "            data[col]=data[col].astype(object)\n",
    "            data.rename(columns={f'{col}':f'hole_id'},inplace=True)\n",
    "        try: \n",
    "            data[col]=data[col].str.strip()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f'{col}: {e}')\n",
    "        data.rename(columns={f'{col}':f'{col.strip().lower()}'},inplace=True)\n",
    "        data=data.drop(data.filter(like='unnamed'),axis=1)\n",
    "        if 'from_ft'in col.lower():\n",
    "            try:\n",
    "                drop_index=data[data['from_ft'].isna()].index\n",
    "                data=data.drop(drop_index,axis=0)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                #print(f'{e}')\n",
    "    if 'hole_id' not in data.columns:\n",
    "        data['hole_id']=np.nan\n",
    "    if 'recvd wt.'  in data.columns:\n",
    "        try:\n",
    "            data['recvd wt.']=pd.to_numeric(data['recvd wt.'],errors='coerce')\n",
    "            data['from_ft']=pd.to_numeric(data['from_ft'],errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    names=data.loc[:,data.columns.duplicated()].columns\n",
    "    if len(names)>0:\n",
    "        print(f'duplcated column: {names} drop or else the nasty merge bug')\n",
    "        data=data.loc[:,~data.columns.duplicated()].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_concat_onid(big_df,data,merge_col=['geo','sample_id','hole_id','from_ft','to_ft']):\n",
    "    \n",
    "    num_common=big_df[big_df.sample_id.isin(data.sample_id)].shape[0]\n",
    "    print(f'{num_common} common ids, concat')\n",
    "\n",
    "    \n",
    "    if num_common==0:\n",
    "        print(f'{num_common} common ids, concat')\n",
    "        big_df=pd.concat([big_df,data],axis=0)\n",
    "    else: \n",
    "        print (f'{num_common} common ids, merge')\n",
    "        big_df=big_df.merge(data,on=merge_col,how='left')\n",
    "    n_col=big_df.shape[1]\n",
    "    print('duplicate columns: ', big_df.columns.duplicated().sum())\n",
    "    print('duplicate indexs: ', big_df.index.duplicated().sum())\n",
    "    print(f'{n_col} columns')\n",
    "    return big_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### merge: drill assays master.csv #######\n",
      "'from_ft'\n",
      "common columns []\n",
      "0 common ids concat\n",
      "data shape (5411, 62)\n",
      "####### merge: spectral master.csv #######\n"
     ]
    },
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000029?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m file \u001b[39min\u001b[39;00m config\u001b[39m.\u001b[39mmerge_samples:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000029?line=4'>5</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m####### merge: \u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m #######\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000029?line=5'>6</a>\u001b[0m     data\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39;49mread_csv(file)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000029?line=6'>7</a>\u001b[0m     data\u001b[39m=\u001b[39mclean_for_merge(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000029?line=7'>8</a>\u001b[0m     data_list\u001b[39m.\u001b[39mappend(data)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    666\u001b[0m     dialect,\n\u001b[1;32m    667\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[1;32m    677\u001b[0m )\n\u001b[1;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[1;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[1;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    930\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m    932\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/readers.py:1235\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1232\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(msg)\n\u001b[1;32m   1234\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1235\u001b[0m     \u001b[39mreturn\u001b[39;00m mapping[engine](f, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions)\n\u001b[1;32m   1236\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1237\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/io/parsers/c_parser_wrapper.py:75\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     72\u001b[0m     kwds\u001b[39m.\u001b[39mpop(key, \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m     74\u001b[0m kwds[\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m ensure_dtype_objs(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mdtype\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m---> 75\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader \u001b[39m=\u001b[39m parsers\u001b[39m.\u001b[39;49mTextReader(src, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwds)\n\u001b[1;32m     77\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munnamed_cols \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reader\u001b[39m.\u001b[39munnamed_cols\n\u001b[1;32m     79\u001b[0m \u001b[39m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/_libs/parsers.pyx:551\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "big_df=pd.DataFrame()\n",
    "data_list=[]\n",
    "prev_cols=[]\n",
    "for file in config.merge_samples:\n",
    "    print(f\"####### merge: {file.split('/')[-1]} #######\")\n",
    "    data=pd.read_csv(file)\n",
    "    data=clean_for_merge(data)\n",
    "    data_list.append(data)\n",
    "    data.astype(object)\n",
    "    data.astype(object)\n",
    "    big_cols=list(big_df.columns)\n",
    "    small_cols=list(data.columns)\n",
    "    \n",
    "    dups=sorted([col for col in big_cols if col in small_cols])\n",
    "    \n",
    "    try:\n",
    "        [dups.remove(x) for x in ['file','folder','start_depth','end_depth']if x in data.columns]\n",
    "    except:\n",
    "        pass\n",
    "    print(f'common columns {dups}')\n",
    "\n",
    "\n",
    "    \n",
    "    big_index=list(big_df.index)\n",
    "    small_index=list(data.index)\n",
    "    \n",
    "    same_ids= [id for id in big_index if id in small_index]\n",
    "    n_ids=len(same_ids)\n",
    "    if n_ids==0:\n",
    "        print(f'{n_ids} common ids concat')\n",
    "        big_df=pd.concat([big_df,data],axis=0,join='outer')\n",
    "    else:\n",
    "        print(f'{n_ids} common ids merge')\n",
    "        big_df=big_df.merge(data,on=dups,how='outer')\n",
    "    print(f'data shape {big_df.shape}')\n",
    "cols=config.main_columns\n",
    "not_dups=[i for i in big_df.columns if i not in cols]\n",
    "big_df=pd.concat([big_df[cols],big_df[not_dups]],axis=1).set_index('sample_id')\n",
    "big_df.sort_index(inplace=True)\n",
    "big_df=big_df.groupby(['sample_id']).apply(lambda x: x.fillna(method='ffill').fillna(method='bfill').drop_duplicates(keep='first')).droplevel(level=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.to_excel('all_sampleid_test.xlsx')\n",
    "big_df.to_excel('/Volumes/GoogleDrive/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/all_sampleid_test.xlsx')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df=pd.read_excel('all_sampleid_test.xlsx')\n",
    "big_df[['from_ft','to_ft']]=big_df[['from_ft','to_ft']].fillna(0)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lith=pd.read_csv(fconfig.lith_file)\n",
    "lith=clean_for_merge(lith)\n",
    "lith"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols=['sample_id','hole_id','from_ft','to_ft']\n",
    "other_cols=[col for col in big_df.columns if col not in merge_cols]\n",
    "zipped=zip(big_df['sample_id'],big_df['hole_id'], big_df['from_ft'], big_df['to_ft'])\n",
    "depths=pd.DataFrame([(s_id,h_id, y) for s_id,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                   columns=['sample_id','hole_id','depth'])\n",
    "depths=depths.merge(big_df,on='sample_id',validate='m:m')\n",
    "depths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=depths.merge(lith,left_on=['hole_id','depth'],right_on=['hole_id','from_ft'],how='outer')\n",
    "other_cols=[col for col in depths if col not in merge_cols+'depths']\n",
    "merged=pd.concat([merged[merge_cols+'depth'],merged[other_cols]],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_excel('merged_test.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
