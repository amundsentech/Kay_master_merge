{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for pandas\n",
      "pandas is properly installed\n",
      "check for tqdm\n",
      "tqdm is properly installed\n",
      "check for numpy\n",
      "numpy is properly installed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cleaningtools as ct \n",
    "import file_config as fconfig\n",
    "import merge_config as config\n",
    "import assay_sample_config as aconfig\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import file_config as fconfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_merge(data):\n",
    "    for col in data.columns:\n",
    "        if 'depth_ft'in col.lower():\n",
    "            data['from_ft']=data[col]\n",
    "            data['to_ft']=data['from_ft'].shift(-1)-1\n",
    "            #convert ft to meters\n",
    "            data['from_m']=data['from_ft']* .3281\n",
    "            data['to_m']=data['to_ft'] * .3281\n",
    "            data=data.drop(data.filter(like='Depth').columns,axis=1)\n",
    "        if ('hole' in col.lower())& ('id' in col.lower()):\n",
    "            data[col]=data[col].astype(object)\n",
    "            data.rename(columns={f'{col}':f'hole_id'},inplace=True)\n",
    "        try: \n",
    "            data[col]=data[col].str.strip()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f'{col}: {e}')\n",
    "        data.rename(columns={f'{col}':f'{col.strip().lower()}'},inplace=True)\n",
    "        data=data.drop(data.filter(like='unnamed'),axis=1)\n",
    "        if 'from_ft'in col.lower():\n",
    "            try:\n",
    "                drop_index=data[data['from_ft'].isna()].index\n",
    "                data=data.drop(drop_index,axis=0)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                #print(f'{e}')\n",
    "    if 'hole_id' not in data.columns:\n",
    "        data['hole_id']=np.nan\n",
    "    if 'recvd wt.'  in data.columns:\n",
    "        try:\n",
    "            data['recvd wt.']=pd.to_numeric(data['recvd wt.'],errors='coerce')\n",
    "            data['from_ft']=pd.to_numeric(data['from_ft'],errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    names=data.loc[:,data.columns.duplicated()].columns\n",
    "    if len(names)>0:\n",
    "        print(f'duplcated column: {names} drop or else the nasty merge bug')\n",
    "        data=data.loc[:,~data.columns.duplicated()].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_merge_groups(data):\n",
    "    data=data.fillna(method='ffill')\n",
    "    data=data.fillna(method='bfill')\n",
    "    data=data.drop_duplicates(keep='first')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_depths(data):\n",
    "    data[['from_ft','to_ft']]=data[['from_ft','to_ft']].fillna(0)\n",
    "    if 'sample_id' in data.columns:\n",
    "        zipped=zip(data['sample_id'],data['hole_id'], data['from_ft'], data['to_ft'])\n",
    "        depths=pd.DataFrame([(s_id,h_id, y) for s_id,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                    columns=['sample_id','hole_id','depth'])\n",
    "        d_frame=depths.merge(data,on=['sample_id','hole_id'])\n",
    "        d_frame=depths.merge(data,on=['sample_id','hole_id'])\n",
    "    else:\n",
    "        data['START']=data['from_ft']\n",
    "        zipped=zip(data['START'],data['hole_id'], data['from_ft'], data['to_ft'])\n",
    "        depths=pd.DataFrame([(s,h_id, y) for s,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                        columns=['START','hole_id','depth'])\n",
    "        d_frame=depths.merge(data,on=['START','hole_id'])\n",
    "        d_frame=d_frame.drop('START',axis=1)\n",
    "    d_frame=d_frame.drop(d_frame.filter(like='from').columns,axis=1)\n",
    "    d_frame=d_frame.drop(d_frame.filter(like='to').columns,axis=1)\n",
    "    return d_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_start_end(data):\n",
    "    depths=data['depth']\n",
    "    start=depths[0]\n",
    "    end=depths[-1]\n",
    "    data['from_ft']=start\n",
    "    data['to_ft']=end\n",
    "    data=data.drop('depth')\n",
    "    data=data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read alteration master\n",
      "clean alteration master\n",
      "alteration master does not have sample ids\n",
      "explode alteration master\n",
      "read mineralization master\n",
      "clean mineralization master\n",
      "duplcated column: Index(['min_type'], dtype='object') drop or else the nasty merge bug\n",
      "mineralization master does not have sample ids\n",
      "explode mineralization master\n",
      "read hyp-package samples master\n",
      "clean hyp-package samples master\n",
      "hyp-package samples master has sample ids\n",
      "read structure master\n",
      "clean structure master\n",
      "structure master does not have sample ids\n",
      "explode structure master\n",
      "read xrf samples master\n",
      "clean xrf samples master\n",
      "xrf samples master does not have sample ids\n",
      "explode xrf samples master\n",
      "\"None of [Index(['from_ft', 'to_ft'], dtype='object')] are in the [columns]\"\n",
      "must have (depths and hole ids) or (sample_ids) to merge into master\n",
      "read drill assay samples master\n",
      "clean drill assay samples master\n",
      "drill assay samples master has sample ids\n",
      "read spectral master\n",
      "clean spectral master\n",
      "'from_ft'\n",
      "duplcated column: Index(['spectrumid'], dtype='object') drop or else the nasty merge bug\n",
      "spectral master has sample ids\n",
      "read drill assays master\n",
      "clean drill assays master\n",
      "'from_ft'\n",
      "drill assays master has sample ids\n",
      "read lithology master\n",
      "clean lithology master\n",
      "lithology master does not have sample ids\n",
      "explode lithology master\n"
     ]
    }
   ],
   "source": [
    "hole_list=[]\n",
    "sample_list=[]\n",
    "sample_data_names=[]\n",
    "hole_data_names=[]\n",
    "files=[file for file in os.listdir(fconfig.output_path) if file.endswith('.csv')]\n",
    "for file in files:\n",
    "    name=file.split('.')[0]\n",
    "    if len(name)==0:\n",
    "        continue\n",
    "    print(f'read {name}')\n",
    "    try:\n",
    "        data=pd.read_csv(fconfig.output_path+file,low_memory=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('likely not a csv')\n",
    "        continue\n",
    "    print(f'clean {name}')\n",
    "    data=clean_for_merge(data)\n",
    "\n",
    "    if 'sample_id' in data.columns:\n",
    "        print(f'{name} has sample ids')\n",
    "        sample_list.append(data)\n",
    "        sample_data_names.append(name)\n",
    "    else:\n",
    "        print(f'{name} does not have sample ids')\n",
    "        print(f'explode {name}')\n",
    "        try:\n",
    "            data=explode_depths(data)\n",
    "            hole_list.append(data)\n",
    "            hole_data_names.append(name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('must have (depths and hole ids) or (sample_ids) to merge into master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data in each group. holes and samples\n",
    "def merge_dflist(data_list,data_names):\n",
    "    big_df=pd.DataFrame()\n",
    "    non_id_cols=['depth', 'hole_id']\n",
    "    for i,data in enumerate(data_list):\n",
    "        ## merge all the data with sample_ids\n",
    "        name=data_names[i]\n",
    "        print(f\"####### merge: {name} #######\")\n",
    "        suf=name.split(' ')[0]\n",
    "        data=data.astype(object)\n",
    "        big_df=big_df.astype(object)\n",
    "        big_cols=list(big_df.columns)\n",
    "        small_cols=list(data.columns)\n",
    "        \n",
    "        dups=sorted([col for col in big_cols if col in small_cols])\n",
    "        \n",
    "        try:\n",
    "            [dups.remove(x) for x in ['file','folder','start_depth','end_depth']if x in data.columns]\n",
    "        except:\n",
    "            pass\n",
    "        if 'depth' in data.columns:\n",
    "            big_index=list(big_df.groupby(non_id_cols).index)\n",
    "            small_index=list(data.groupby(non_id_cols).index)\n",
    "            same_ids= [id for id in big_index if id in small_index]\n",
    "            dups=non_id_cols\n",
    "        else:\n",
    "            big_index=list(big_df.index)\n",
    "            small_index=list(data.index)\n",
    "            same_ids= [id for id in big_index if id in small_index]\n",
    "        print(f'common columns {dups}')\n",
    "        n_ids=len(same_ids)\n",
    "        if n_ids==0:\n",
    "            print(f'{n_ids} common ids concat')\n",
    "            big_df=pd.concat([big_df,data],axis=0,join='outer')\n",
    "        else:\n",
    "            print(f'{n_ids} common ids merge')\n",
    "            big_df=big_df.merge(data,on=non_id_cols,suffixes=[None,suf],how='outer')\n",
    "\n",
    "        print(f'data shape {big_df.shape}')\n",
    "    cols=config.main_columns\n",
    "    not_dups=[i for i in big_df.columns if i not in cols]\n",
    "    big_final=pd.concat([big_df[cols],big_df[not_dups]],axis=1)\n",
    "\n",
    "    big_final.drop(big_final.filter(like='file').columns,axis=1,inplace=True)\n",
    "    big_final.drop(big_final.filter(like='folder').columns,axis=1,inplace=True)\n",
    "    big_final.sort_index(inplace=True)\n",
    "    return big_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### merge: hyp-package samples master #######\n",
      "common columns []\n",
      "0 common ids concat\n",
      "data shape (1080, 9)\n",
      "####### merge: drill assay samples master #######\n",
      "common columns ['from_ft', 'from_m', 'geo', 'hole_id', 'sample_id', 'to_ft', 'to_m']\n",
      "1077 common ids merge\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'depth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb Cell 8'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000026?line=0'>1</a>\u001b[0m big_samples\u001b[39m=\u001b[39mmerge_dflist(sample_list,sample_data_names)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000026?line=1'>2</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mFINISHED MERGING SAMPLES\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000026?line=2'>3</a>\u001b[0m big_holes\u001b[39m=\u001b[39mmerge_dflist(hole_list,hole_data_names)\n",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb Cell 7'\u001b[0m in \u001b[0;36mmerge_dflist\u001b[0;34m(data_list, data_names)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000025?line=34'>35</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000025?line=35'>36</a>\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mn_ids\u001b[39m}\u001b[39;00m\u001b[39m common ids merge\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000025?line=36'>37</a>\u001b[0m         big_df\u001b[39m=\u001b[39mbig_df\u001b[39m.\u001b[39;49mmerge(data,on\u001b[39m=\u001b[39;49mnon_id_cols,suffixes\u001b[39m=\u001b[39;49m[\u001b[39mNone\u001b[39;49;00m,suf],how\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mouter\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000025?line=38'>39</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mdata shape \u001b[39m\u001b[39m{\u001b[39;00mbig_df\u001b[39m.\u001b[39mshape\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper.ipynb#ch0000025?line=39'>40</a>\u001b[0m cols\u001b[39m=\u001b[39mconfig\u001b[39m.\u001b[39mmain_columns\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:9345\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9326\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   9327\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m   9328\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9341\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   9342\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[1;32m   9343\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mreshape\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmerge\u001b[39;00m \u001b[39mimport\u001b[39;00m merge\n\u001b[0;32m-> 9345\u001b[0m     \u001b[39mreturn\u001b[39;00m merge(\n\u001b[1;32m   9346\u001b[0m         \u001b[39mself\u001b[39;49m,\n\u001b[1;32m   9347\u001b[0m         right,\n\u001b[1;32m   9348\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m   9349\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m   9350\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m   9351\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m   9352\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m   9353\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m   9354\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m   9355\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m   9356\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m   9357\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m   9358\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m   9359\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:107\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[39m@Substitution\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mleft : DataFrame or named Series\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[39m@Appender\u001b[39m(_merge_doc, indents\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     validate: \u001b[39mstr\u001b[39m \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame:\n\u001b[0;32m--> 107\u001b[0m     op \u001b[39m=\u001b[39m _MergeOperation(\n\u001b[1;32m    108\u001b[0m         left,\n\u001b[1;32m    109\u001b[0m         right,\n\u001b[1;32m    110\u001b[0m         how\u001b[39m=\u001b[39;49mhow,\n\u001b[1;32m    111\u001b[0m         on\u001b[39m=\u001b[39;49mon,\n\u001b[1;32m    112\u001b[0m         left_on\u001b[39m=\u001b[39;49mleft_on,\n\u001b[1;32m    113\u001b[0m         right_on\u001b[39m=\u001b[39;49mright_on,\n\u001b[1;32m    114\u001b[0m         left_index\u001b[39m=\u001b[39;49mleft_index,\n\u001b[1;32m    115\u001b[0m         right_index\u001b[39m=\u001b[39;49mright_index,\n\u001b[1;32m    116\u001b[0m         sort\u001b[39m=\u001b[39;49msort,\n\u001b[1;32m    117\u001b[0m         suffixes\u001b[39m=\u001b[39;49msuffixes,\n\u001b[1;32m    118\u001b[0m         copy\u001b[39m=\u001b[39;49mcopy,\n\u001b[1;32m    119\u001b[0m         indicator\u001b[39m=\u001b[39;49mindicator,\n\u001b[1;32m    120\u001b[0m         validate\u001b[39m=\u001b[39;49mvalidate,\n\u001b[1;32m    121\u001b[0m     )\n\u001b[1;32m    122\u001b[0m     \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:700\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cross \u001b[39m=\u001b[39m cross_col\n\u001b[1;32m    695\u001b[0m \u001b[39m# note this function has side effects\u001b[39;00m\n\u001b[1;32m    696\u001b[0m (\n\u001b[1;32m    697\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mleft_join_keys,\n\u001b[1;32m    698\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mright_join_keys,\n\u001b[1;32m    699\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mjoin_names,\n\u001b[0;32m--> 700\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_merge_keys()\n\u001b[1;32m    702\u001b[0m \u001b[39m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[39m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1097\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1095\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m is_rkey(rk):\n\u001b[1;32m   1096\u001b[0m     \u001b[39mif\u001b[39;00m rk \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1097\u001b[0m         right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39;49m_get_label_or_level_values(rk))\n\u001b[1;32m   1098\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1099\u001b[0m         \u001b[39m# work-around for merge_asof(right_index=True)\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m         right_keys\u001b[39m.\u001b[39mappend(right\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:1840\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maxes[axis]\u001b[39m.\u001b[39mget_level_values(key)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1839\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1842\u001b[0m \u001b[39m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[39mif\u001b[39;00m values\u001b[39m.\u001b[39mndim \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'depth'"
     ]
    }
   ],
   "source": [
    "\n",
    "big_samples=merge_dflist(sample_list,sample_data_names)\n",
    "print('FINISHED MERGING SAMPLES')\n",
    "big_holes=merge_dflist(hole_list,hole_data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#####clean the sample_ids#########')\n",
    "big_samples=big_samples.groupby(['sample_id']).progress_apply(fill_merge_groups).droplevel(level=0)\n",
    "print('#####explode sample_ids#########')\n",
    "samp_ex=explode_depths(big_samples)\n",
    "print('#########merge holes and samples##########')\n",
    "merged=samp_ex.merge(big_holes,on=['hole_id','depth'],how='outer')\n",
    "print('clean the final_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop('depth',axis=1).duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=merged.groupby(['hole_id','depth']).progress_apply(fill_merge_groups).droplevel(level=0)\n",
    "\n",
    "merge_cols=['sample_id','hole_id','depth']\n",
    "other_cols=[col for col in merged.columns if col not in merge_cols]\n",
    "merged=pd.concat([merged[merge_cols],merged[other_cols]],axis=1)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df=pd.DataFrame()\n",
    "data_list=[]\n",
    "prev_cols=[]\n",
    "for file in config.merge_samples:\n",
    "    print(f\"####### merge: {file.split('/')[-1]} #######\")\n",
    "    data=pd.read_csv(file,low_memory=False)\n",
    "    data=clean_for_merge(data)\n",
    "    \n",
    "    data=data.astype(object)\n",
    "    data_list.append(data)\n",
    "    big_df=big_df.astype(object)\n",
    "    big_cols=list(big_df.columns)\n",
    "    small_cols=list(data.columns)\n",
    "    \n",
    "    dups=sorted([col for col in big_cols if col in small_cols])\n",
    "    \n",
    "    try:\n",
    "        [dups.remove(x) for x in ['file','folder','start_depth','end_depth']if x in data.columns]\n",
    "    except:\n",
    "        pass\n",
    "    print(f'common columns {dups}')\n",
    "\n",
    "    data_list.append(data)\n",
    "    \n",
    "    big_index=list(big_df.index)\n",
    "    small_index=list(data.index)\n",
    "    \n",
    "    same_ids= [id for id in big_index if id in small_index]\n",
    "    n_ids=len(same_ids)\n",
    "    if n_ids==0:\n",
    "        print(f'{n_ids} common ids concat')\n",
    "        big_df=pd.concat([big_df,data],axis=0,join='outer')\n",
    "    else:\n",
    "        print(f'{n_ids} common ids merge')\n",
    "        big_df=big_df.merge(data,on=dups,how='outer')\n",
    "    print(f'data shape {big_df.shape}')\n",
    "cols=config.main_columns\n",
    "not_dups=[i for i in big_df.columns if i not in cols]\n",
    "\n",
    "big_df=pd.concat([big_df[cols],big_df[not_dups]],axis=1).set_index('sample_id')\n",
    "big_df.sort_index(inplace=True)\n",
    "big_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df=big_df.groupby(['sample_id']).progress_apply(fill_merge_groups).droplevel(level=0)\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df.drop(big_df.filter(like='file').columns,axis=1,inplace=True)\n",
    "big_df.drop(big_df.filter(like='folder').columns,axis=1,inplace=True)\n",
    "big_df.to_excel('all_sampleid_test.xlsx')\n",
    "#big_df.to_excel('/Volumes/GoogleDrive/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/all_sampleid_test.xlsx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_df=pd.read_excel('all_sampleid_test.xlsx')\n",
    "big_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lith=pd.read_csv(fconfig.lith_file)\n",
    "lith=clean_for_merge(lith)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_ex=explode_depths(big_df)\n",
    "samp_ex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lith_ex=explode_depths(lith)\n",
    "lith_ex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=samp_ex.merge(lith_ex,on=['hole_id','depth'],how='outer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols=['sample_id','hole_id','depth']\n",
    "other_cols=[col for col in merged.columns if col not in merge_cols]\n",
    "merged=pd.concat([merged[merge_cols],merged[other_cols]],axis=1)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.groupby(['hole_id','depth']).progress_apply(fill_merge_groups).droplevel(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_list=[]\n",
    "sample_list=[]\n",
    "sample_data_names=[]\n",
    "hole_data_names=[]\n",
    "files=[file for file in os.listdir(fconfig.output_path) if file.endswith('.csv')]\n",
    "for file in files:\n",
    "    name=file.split('.')[0]\n",
    "    print(f'read {name}')\n",
    "    try:\n",
    "        data=pd.read_csv(fconfig.output_path+file,low_memory=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('likely not a csv')\n",
    "        continue\n",
    "    data=clean_for_merge(data)\n",
    "\n",
    "    if 'sample_id' in data.columns:\n",
    "        print(f'{name} has sample ids')\n",
    "        sample_list.append(data)\n",
    "        sample_data_names.append(name)\n",
    "    else:\n",
    "        print(f'{name} does not have sample ids')\n",
    "        print(f'explode {name}')\n",
    "        data=explode_depths(data)\n",
    "        hole_list.append(data)\n",
    "        hole_data_names.append(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.drop('depth',axis=1).duplicated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_cols=['sample_id','hole_id','from_ft','to_ft']\n",
    "other_cols=[col for col in merged.columns if col not in merge_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_list=[]\n",
    "sample_list=[]\n",
    "sample_data_names=[]\n",
    "hole_data_names=[]\n",
    "for file in fconfig.output_path:\n",
    "    name=file.split('.')[0]\n",
    "    print(f'read {name}')\n",
    "    try:\n",
    "        data=pd.read_csv(file,low_memory=False)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('likely not a csv')\n",
    "        continue\n",
    "    data=clean_for_merge(data)\n",
    "\n",
    "    if 'sample_id' in data.columns:\n",
    "        print(f'{name} has sample ids')\n",
    "        sample_list.append(data)\n",
    "        sample_data_names.append(name)\n",
    "    else:\n",
    "        print(f'{name} does not have sample ids')\n",
    "        print(f'explode {name}')\n",
    "        data=explode_depths(data)\n",
    "        hole_list.append(data)\n",
    "        hole_data_names.append(name)\n",
    "#### read all the data curations and pick out the ones with sample_ids\n",
    "\n",
    "# merge the data in each group. holes and samples\n",
    "def merge_dflist(data_list,data_names):\n",
    "    big_df=pd.DataFrame()\n",
    "    for i,data in enumerate(data_list):\n",
    "        ## merge all the data with sample_ids\n",
    "        print(f\"####### merge: {data_names[i]} #######\")\n",
    "        data=data.astype(object)\n",
    "        big_df=big_df.astype(object)\n",
    "        big_cols=list(big_df.columns)\n",
    "        small_cols=list(data.columns)\n",
    "        \n",
    "        dups=sorted([col for col in big_cols if col in small_cols])\n",
    "        \n",
    "        try:\n",
    "            [dups.remove(x) for x in ['file','folder','start_depth','end_depth']if x in data.columns]\n",
    "        except:\n",
    "            pass\n",
    "        print(f'common columns {dups}')\n",
    "        big_index=list(big_df.index)\n",
    "        small_index=list(data.index)\n",
    "        \n",
    "        same_ids= [id for id in big_index if id in small_index]\n",
    "        n_ids=len(same_ids)\n",
    "        if n_ids==0:\n",
    "            print(f'{n_ids} common ids concat')\n",
    "            big_df=pd.concat([big_df,data],axis=0,join='outer')\n",
    "        else:\n",
    "            print(f'{n_ids} common ids merge')\n",
    "            big_df=big_df.merge(data,on=dups,how='outer')\n",
    "        print(f'data shape {big_df.shape}')\n",
    "    cols=config.main_columns\n",
    "    if 'sample_id' not in data.columns:\n",
    "        cols.remove['sample_id']\n",
    "        not_dups=[i for i in big_df.columns if i not in cols]\n",
    "        pd.concat([big_df[cols],big_df[not_dups]],axis=1).set_index('sample_id')\n",
    "    else:\n",
    "        not_dups=[i for i in big_df.columns if i not in cols]\n",
    "        big_final=pd.concat([big_df[cols],big_df[not_dups]],axis=1)\n",
    "\n",
    "    big_final.drop(big_final.filter(like='file').columns,axis=1,inplace=True)\n",
    "    big_final.drop(big_final.filter(like='folder').columns,axis=1,inplace=True)\n",
    "    big_final.sort_index(inplace=True)\n",
    "    return big_final\n",
    "\n",
    "\n",
    "big_samples=merge_dflist(sample_list,sample_data_names)\n",
    "big_holes=merge_dflist(hole_list,hole_data_names)\n",
    "print('clean the sample_ids')\n",
    "big_samples=big_samples.groupby(['sample_id']).progress_apply(fill_merge_groups).droplevel(level=0)\n",
    "print('merge holes and samples')\n",
    "merged=big_samples.merge(big_holes,on=['hole_id','depth'],how='outer')\n",
    "print('clean the final_data')\n",
    "merged=merged.groupby(['hole_id','depth']).progress_apply(fill_merge_groups).droplevel(level=0)\n",
    "\n",
    "merge_cols=['sample_id','hole_id','depth']\n",
    "other_cols=[col for col in merged.columns if col not in merge_cols]\n",
    "merged=pd.concat([merged[merge_cols],merged[other_cols]],axis=1)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged.to_excel('/Volumes/GoogleDrive/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/master_MASTER.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
