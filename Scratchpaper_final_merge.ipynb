{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for pandas\n",
      "pandas is properly installed\n",
      "check for tqdm\n",
      "tqdm is properly installed\n",
      "check for numpy\n",
      "numpy is properly installed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cleaningtools as ct \n",
    "import file_config as fconfig\n",
    "import merge_config as config\n",
    "import assay_sample_config as aconfig\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import file_config as fconfig\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_merge(data):\n",
    "    for col in data.columns:\n",
    "        if 'depth_ft'in col.lower():\n",
    "            data['from_ft']=data[col]\n",
    "            data['to_ft']=data['from_ft'].shift(-1)-1\n",
    "            #convert ft to meters\n",
    "            data['from_m']=data['from_ft']* .3281\n",
    "            data['to_m']=data['to_ft'] * .3281\n",
    "            data=data.drop(data.filter(like='Depth').columns,axis=1)\n",
    "        if ('hole' in col.lower())& ('id' in col.lower()):\n",
    "            data[col]=data[col].astype(object)\n",
    "            data.rename(columns={f'{col}':f'hole_id'},inplace=True)\n",
    "        try: \n",
    "            data[col]=data[col].str.strip()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f'{col}: {e}')\n",
    "        data.rename(columns={f'{col}':f'{col.strip().lower()}'},inplace=True)\n",
    "        data=data.drop(data.filter(like='unnamed'),axis=1)\n",
    "        if 'from_ft'in col.lower():\n",
    "            try:\n",
    "                drop_index=data[data['from_ft'].isna()].index\n",
    "                data=data.drop(drop_index,axis=0)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                #print(f'{e}')\n",
    "    if 'hole_id' not in data.columns:\n",
    "        data['hole_id']=np.nan\n",
    "    if 'recvd wt.'  in data.columns:\n",
    "        try:\n",
    "            data['recvd wt.']=pd.to_numeric(data['recvd wt.'],errors='coerce')\n",
    "            data['from_ft']=pd.to_numeric(data['from_ft'],errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    names=data.loc[:,data.columns.duplicated()].columns\n",
    "    if len(names)>0:\n",
    "        print(f'duplcated column: {names} drop or else the nasty merge bug')\n",
    "        data=data.loc[:,~data.columns.duplicated()].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_merge_groups(data):\n",
    "    data=data.fillna(method='ffill')\n",
    "    data=data.fillna(method='bfill')\n",
    "    data=data.drop_duplicates(keep='first')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_depths(data):\n",
    "    data[['from_ft','to_ft']]=data[['from_ft','to_ft']].fillna(0)\n",
    "    if 'sample_id' in data.columns:\n",
    "        zipped=zip(data['sample_id'],data['hole_id'], data['from_ft'], data['to_ft'])\n",
    "        depths=pd.DataFrame([(s_id,h_id, y) for s_id,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                    columns=['sample_id','hole_id','depth'])\n",
    "        d_frame=depths.merge(data,on=['sample_id','hole_id'])\n",
    "        d_frame=depths.merge(data,on=['sample_id','hole_id'])\n",
    "    else:\n",
    "        data['START']=data['from_ft']\n",
    "        zipped=zip(data['START'],data['hole_id'], data['from_ft'], data['to_ft'])\n",
    "        depths=pd.DataFrame([(s,h_id, y) for s,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                        columns=['START','hole_id','depth'])\n",
    "        d_frame=depths.merge(data,on=['START','hole_id'])\n",
    "        d_frame=d_frame.drop('START',axis=1)\n",
    "    d_frame=d_frame.drop(d_frame.filter(like='from').columns,axis=1)\n",
    "    d_frame=d_frame.drop(d_frame.filter(like='to').columns,axis=1)\n",
    "    return d_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_start_end(data):\n",
    "    depths=data['depth']\n",
    "    start=depths[0]\n",
    "    end=depths[-1]\n",
    "    data['from_ft']=start\n",
    "    data['to_ft']=end\n",
    "    data=data.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read alteration master\n",
      "clean alteration master\n",
      "alteration master does not have sample ids\n",
      "explode alteration master\n",
      "read mineralization master\n",
      "clean mineralization master\n",
      "duplcated column: Index(['min_type'], dtype='object') drop or else the nasty merge bug\n",
      "mineralization master does not have sample ids\n",
      "explode mineralization master\n",
      "read hyp-package samples master\n",
      "clean hyp-package samples master\n",
      "hyp-package samples master has sample ids\n",
      "read structure master\n",
      "clean structure master\n",
      "structure master does not have sample ids\n",
      "explode structure master\n",
      "read xrf samples master\n",
      "clean xrf samples master\n",
      "xrf samples master has sample ids\n",
      "read drill assay samples master\n",
      "clean drill assay samples master\n",
      "drill assay samples master has sample ids\n",
      "read drill assays master\n",
      "clean drill assays master\n",
      "'from_ft'\n",
      "drill assays master has sample ids\n",
      "read lithology master\n",
      "clean lithology master\n",
      "lithology master does not have sample ids\n",
      "explode lithology master\n",
      "read spectral master\n",
      "clean spectral master\n",
      "'from_ft'\n",
      "duplcated column: Index(['spectrumid'], dtype='object') drop or else the nasty merge bug\n",
      "spectral master has sample ids\n"
     ]
    }
   ],
   "source": [
    "hole_list=[]\n",
    "sample_list=[]\n",
    "sample_data_names=[]\n",
    "hole_data_names=[]\n",
    "files=[file for file in os.listdir(fconfig.output_path) if file.endswith('.csv')]\n",
    "for file in files:\n",
    "    name=file.split('.')[0]\n",
    "    if len(name)==0:\n",
    "        continue\n",
    "    print(f'read {name}')\n",
    "    try:\n",
    "        data=pd.read_csv(fconfig.output_path+file,low_memory=False,on_bad_lines='skip')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('likely not a csv')\n",
    "        continue\n",
    "    print(f'clean {name}')\n",
    "    data=clean_for_merge(data)\n",
    "\n",
    "    if 'sample_id' in data.columns:\n",
    "        print(f'{name} has sample ids')\n",
    "        sample_list.append(data)\n",
    "        sample_data_names.append(name)\n",
    "    else:\n",
    "        print(f'{name} does not have sample ids')\n",
    "        print(f'explode {name}')\n",
    "        try:\n",
    "            data=explode_depths(data)\n",
    "            hole_list.append(data)\n",
    "            hole_data_names.append(name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('must have (depths and hole ids) or (sample_ids) to merge into master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data in each group. holes and samples\n",
    "def merge_dflist(data_list,data_names):\n",
    "    big_df=pd.DataFrame()\n",
    "    non_id_cols=['depth', 'hole_id']\n",
    "    for i,data in enumerate(data_list):\n",
    "        ## merge all the data with sample_ids\n",
    "        name=data_names[i]\n",
    "        suf=name.split(' ')[0]\n",
    "        print(f\"####### merge frame {i}: {name} suffix= {suf} #######\")\n",
    "        data=data.astype(object)\n",
    "        big_df=big_df.astype(object)\n",
    "        big_cols=list(big_df.columns)\n",
    "        small_cols=list(data.columns)\n",
    "        \n",
    "        dups=sorted([col for col in big_cols if col in small_cols])\n",
    "        cols=config.main_columns\n",
    "        try:\n",
    "            [dups.remove(x) for x in ['file','folder','start_depth','end_depth']if x in data.columns]\n",
    "        except:\n",
    "            pass\n",
    "        if 'sample_id' not in data.columns:\n",
    "            cols=non_id_cols\n",
    "            try:\n",
    "                big_index=list(big_df.hole_id.unique())\n",
    "                small_index=list(data.hole_id.unique())\n",
    "                same_ids= [id for id in big_index if id in small_index]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                same_ids=[]\n",
    "            dups=non_id_cols\n",
    "        else:\n",
    "            big_index=list(big_df.index)\n",
    "            small_index=list(data.index)\n",
    "            same_ids= [id for id in big_index if id in small_index]\n",
    "        print(f'common columns {dups}')\n",
    "        n_ids=len(same_ids)\n",
    "        if n_ids==0:\n",
    "            print(f'{n_ids} common ids concat')\n",
    "            big_df=pd.concat([big_df,data],axis=0,join='outer')\n",
    "        else:\n",
    "            print(f'{n_ids} common ids merge')\n",
    "            big_df=big_df.merge(data,on=dups,suffixes=[None,'_'+suf],how='outer')\n",
    "\n",
    "        print(f'data shape {big_df.shape}')\n",
    "    \n",
    "    not_dups=[i for i in big_df.columns if i not in cols]\n",
    "    big_final=pd.concat([big_df[cols],big_df[not_dups]],axis=1)\n",
    "\n",
    "    big_final.drop(big_final.filter(like='file').columns,axis=1,inplace=True)\n",
    "    big_final.drop(big_final.filter(like='folder').columns,axis=1,inplace=True)\n",
    "    big_final.sort_index(inplace=True)\n",
    "    return big_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "####### merge frame 0: hyp-package samples master suffix= hyp-package #######\n",
      "common columns []\n",
      "0 common ids concat\n",
      "data shape (1080, 9)\n",
      "####### merge frame 1: xrf samples master suffix= xrf #######\n",
      "common columns ['hole_id', 'sample_id']\n",
      "432 common ids merge\n",
      "data shape (1512, 186)\n",
      "####### merge frame 2: drill assay samples master suffix= drill #######\n",
      "common columns ['from_ft', 'from_m', 'geo', 'hole_id', 'qaqc', 'sample_id', 'to_ft', 'to_m']\n",
      "1509 common ids merge\n",
      "data shape (3103, 194)\n",
      "####### merge frame 3: drill assays master suffix= drill #######\n",
      "common columns ['hole_id', 'sample_id']\n",
      "3103 common ids merge\n",
      "data shape (8514, 254)\n",
      "####### merge frame 4: spectral master suffix= spectral #######\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:916: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  key_col = Index(lvals).where(~mask_left, rvals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "common columns ['hole_id', 'recvd wt.', 'sample_id']\n",
      "8044 common ids merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:916: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  key_col = Index(lvals).where(~mask_left, rvals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (16080, 377)\n",
      "FINISHED MERGING SAMPLES\n",
      "####### merge frame 0: alteration master suffix= alteration #######\n",
      "'DataFrame' object has no attribute 'hole_id'\n",
      "common columns ['depth', 'hole_id']\n",
      "0 common ids concat\n",
      "data shape (109179, 13)\n",
      "####### merge frame 1: mineralization master suffix= mineralization #######\n",
      "common columns ['depth', 'hole_id']\n",
      "62 common ids merge\n",
      "data shape (118186, 24)\n",
      "####### merge frame 2: structure master suffix= structure #######\n",
      "common columns ['depth', 'hole_id']\n",
      "82 common ids merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:916: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  key_col = Index(lvals).where(~mask_left, rvals)\n",
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:916: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  key_col = Index(lvals).where(~mask_left, rvals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (145555, 32)\n",
      "####### merge frame 3: lithology master suffix= lithology #######\n",
      "common columns ['depth', 'hole_id']\n",
      "89 common ids merge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:916: FutureWarning: In a future version, the Index constructor will not infer numeric dtypes when passed object-dtype sequences (matching Series behavior)\n",
      "  key_col = Index(lvals).where(~mask_left, rvals)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data shape (360906, 40)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "big_samples=merge_dflist(sample_list,sample_data_names)\n",
    "print('FINISHED MERGING SAMPLES')\n",
    "big_holes=merge_dflist(hole_list,hole_data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#####clean the sample_ids#########\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9e519a51c3e463abdb102c47f147666",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9377 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('#####clean the sample_ids#########')\n",
    "big_samples=big_samples.groupby(['sample_id']).progress_apply(fill_merge_groups).droplevel(level=0)\n",
    "print('#####explode sample_ids#########')\n",
    "samp_ex=explode_depths(big_samples)\n",
    "print('#########merge holes and samples##########')\n",
    "merged=samp_ex.merge(big_holes,on=['hole_id','depth'],how='outer')\n",
    "print('clean the final_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dups=merged[merged.drop('depth',axis=1).duplicated(keep=False)]\n",
    "dups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged=merged.groupby(['hole_id','depth']).progress_apply(fill_merge_groups).reset_index(drop=True)\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dup_cols=list( merged.columns)\n",
    "dup_cols.remove('depth')\n",
    "dup_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['u_id']=merged.groupby(dup_cols).ngroup()\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test=merged.groupby(merged['u_id']).progress_apply(pull_start_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.drop('depth',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.to_excel('/Volumes/GoogleDrive/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/master_MASTER.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
