{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check for pandas\n",
      "pandas is properly installed\n",
      "check for tqdm\n",
      "tqdm is properly installed\n",
      "check for numpy\n",
      "numpy is properly installed\n",
      "check for openpyxl\n",
      "openpyxl is properly installed\n",
      "check for xlwings\n",
      "xlwings is properly installed\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cleaningtools as ct \n",
    "import file_config as fconfig\n",
    "import curate_config as config\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm.notebook import tqdm_notebook as tqdm\n",
    "tqdm.pandas()\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import file_config as fconfig\n",
    "import pandas as pd\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir='/Users/sebastiancoombs/Library/CloudStorage/GoogleDrive-sebastianbcoombs@gmail.com/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/'\n",
    "files= os.listdir(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['geochemical', 'hyp-pkg', 'terraspec', 'xrf'] ['drill samples master.csv']\n",
      "---------------------------------------------------\n",
      "['drill geochemical assays master.csv', 'drill samples master.csv']\n",
      "drill geochemical\n",
      "Merging drill samples master  with drill geochemical assays master data\n",
      "using columns\n",
      "sample_id ; sample_id_description\n",
      "first columns before sort\n",
      "['sample_id', 'hole_id', 'from_ft', 'to_ft', 'from_m', 'to_m', 'description', 'qaqc', 'source_master', 'ag_ppm', 'ag_ppm(2)', 'al_%', 'as_ppm', 'au_ppm', 'au_ppm(2)']\n",
      "first columns after sort\n",
      "['sample_id', 'hole_id', 'from_ft', 'to_ft', 'from_m', 'to_m', 'description', 'qaqc', 'source_master', 'ag_ppm', 'ag_ppm(2)', 'al_%', 'as_ppm', 'au_ppm', 'au_ppm(2)']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>hole_id</th>\n",
       "      <th>from_ft</th>\n",
       "      <th>to_ft</th>\n",
       "      <th>from_m</th>\n",
       "      <th>to_m</th>\n",
       "      <th>description</th>\n",
       "      <th>qaqc</th>\n",
       "      <th>source_master</th>\n",
       "      <th>ag_ppm</th>\n",
       "      <th>...</th>\n",
       "      <th>th_ppm</th>\n",
       "      <th>ti_%</th>\n",
       "      <th>tl_ppm</th>\n",
       "      <th>u_ppm</th>\n",
       "      <th>v_ppm</th>\n",
       "      <th>w_ppm</th>\n",
       "      <th>y_ppm</th>\n",
       "      <th>zn_%</th>\n",
       "      <th>zn_ppm</th>\n",
       "      <th>zr_ppm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B300015</td>\n",
       "      <td>KM-20-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>STD CDN-ME-1410</td>\n",
       "      <td>KM-20-01.xlsx</td>\n",
       "      <td>71</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0.05</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>10</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>37100</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B300016</td>\n",
       "      <td>KM-20-01</td>\n",
       "      <td>469.5</td>\n",
       "      <td>472.0</td>\n",
       "      <td>143.1</td>\n",
       "      <td>143.9</td>\n",
       "      <td>Chlorite-sericite schist with concordant quart...</td>\n",
       "      <td>0</td>\n",
       "      <td>KM-20-01.xlsx</td>\n",
       "      <td>&lt;1</td>\n",
       "      <td>...</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0.60</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>350</td>\n",
       "      <td>&lt;50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>140</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 67 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id   hole_id  from_ft  to_ft  from_m   to_m  \\\n",
       "0   B300015  KM-20-01      0.0    0.0     0.0    0.0   \n",
       "1   B300016  KM-20-01    469.5  472.0   143.1  143.9   \n",
       "\n",
       "                                         description             qaqc  \\\n",
       "0                                                  0  STD CDN-ME-1410   \n",
       "1  Chlorite-sericite schist with concordant quart...                0   \n",
       "\n",
       "   source_master ag_ppm  ...  th_ppm  ti_% tl_ppm u_ppm  v_ppm w_ppm y_ppm  \\\n",
       "0  KM-20-01.xlsx     71  ...     <50  0.05    <50   <50     10   <50   0.0   \n",
       "1  KM-20-01.xlsx     <1  ...     <50  0.60    <50   <50    350   <50   0.0   \n",
       "\n",
       "  zn_% zn_ppm zr_ppm  \n",
       "0    0  37100    0.0  \n",
       "1    0    140    0.0  \n",
       "\n",
       "[2 rows x 67 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "output location:\n",
      "/Users/sebastiancoombs/Library/CloudStorage/GoogleDrive-sebastianbcoombs@gmail.com/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/drill geochemical_merged.csv\n",
      "---------------------------------------------------\n",
      "['drill hyp-pkg samples master.csv', 'drill hyp-pkg certificates master.csv', 'drill samples master.csv']\n",
      "drill hyp-pkg\n",
      "Merging drill samples master samples with drill hyp-pkg samples master samples\n",
      "using columns\n",
      "sample_id : sample_id\n",
      "Merging drill samples master + drill hyp-pkg samples master with drill hyp-pkg samples master data\n",
      "Error in sorting\n",
      "The column label 'hole_id_samples' is not unique.\n",
      "first columns before sort\n",
      "['sample_id', 'hole_id_samples', 'from_ft', 'to_ft', 'from_m', 'to_m', 'description_samples', 'qaqc', 'source_samples', 'hole_id_samples', 'depth_ft', 'depth_m', 'description_samples', 'source_samples', 'depthfrom']\n",
      "first columns after sort\n",
      "['sample_id', 'from_ft', 'to_ft', 'from_m', 'to_m', 'depth_ft', 'depth_m', 'depthfrom', 'depthto', 'hole_id_samples', 'description_samples', 'qaqc', 'source_samples', 'hole_id_samples', 'description_samples']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>from_ft</th>\n",
       "      <th>to_ft</th>\n",
       "      <th>from_m</th>\n",
       "      <th>to_m</th>\n",
       "      <th>depth_ft</th>\n",
       "      <th>depth_m</th>\n",
       "      <th>depthfrom</th>\n",
       "      <th>depthto</th>\n",
       "      <th>hole_id_samples</th>\n",
       "      <th>...</th>\n",
       "      <th>wd_mgohcb</th>\n",
       "      <th>wd_oh1400</th>\n",
       "      <th>whitemica_sc</th>\n",
       "      <th>xt_kaol2160</th>\n",
       "      <th>xt_kaol2180</th>\n",
       "      <th>xt_kaolinite</th>\n",
       "      <th>xt_wtmica</th>\n",
       "      <th>zeolite_sc</th>\n",
       "      <th>zoisite_sc</th>\n",
       "      <th>zunyite_sc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B300024</td>\n",
       "      <td>1027.0</td>\n",
       "      <td>1032.0</td>\n",
       "      <td>313.0</td>\n",
       "      <td>314.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>KM-20-01</td>\n",
       "      <td>...</td>\n",
       "      <td>45.0</td>\n",
       "      <td>22.59</td>\n",
       "      <td>45.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B300025</td>\n",
       "      <td>1059.0</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>322.8</td>\n",
       "      <td>323.7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>KM-20-01</td>\n",
       "      <td>...</td>\n",
       "      <td>47.8</td>\n",
       "      <td>23.75</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>6.011</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 149 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id  from_ft   to_ft  from_m   to_m  depth_ft  depth_m  depthfrom  \\\n",
       "0   B300024   1027.0  1032.0   313.0  314.6       NaN      NaN        0.0   \n",
       "1   B300025   1059.0  1062.0   322.8  323.7       NaN      NaN        0.0   \n",
       "\n",
       "   depthto hole_id_samples  ... wd_mgohcb wd_oh1400 whitemica_sc xt_kaol2160  \\\n",
       "0      0.0        KM-20-01  ...      45.0     22.59         45.0         0.0   \n",
       "1      0.0        KM-20-01  ...      47.8     23.75         85.0         0.0   \n",
       "\n",
       "  xt_kaol2180 xt_kaolinite xt_wtmica zeolite_sc zoisite_sc zunyite_sc  \n",
       "0         0.0            0     0.000        0.0        0.0        0.0  \n",
       "1         0.0            0     6.011        0.0        0.0        0.0  \n",
       "\n",
       "[2 rows x 149 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "output location:\n",
      "/Users/sebastiancoombs/Library/CloudStorage/GoogleDrive-sebastianbcoombs@gmail.com/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/drill hyp-pkg_merged.csv\n",
      "---------------------------------------------------\n",
      "['drill terraspec spectral master.csv', 'drill terraspec samples master.csv']\n",
      "drill terraspec\n",
      "Merging drill terraspec samples master  with drill terraspec spectral master data\n",
      "using columns\n",
      "file_name ; sample\n",
      "first columns before sort\n",
      "['sample_id', 'hole_id', 'depth_ft', 'date_samples', 'file_name', 'internal_id', 'notes', 'source_samples', 'user', 'alfemg', 'aloh', 'bestelevation', 'bestlatitude', 'bestlongitude', 'by']\n",
      "first columns after sort\n",
      "['sample_id', 'hole_id', 'depth_ft', 'date_samples', 'file_name', 'internal_id', 'notes', 'source_samples', 'user', 'alfemg', 'aloh', 'bestelevation', 'bestlatitude', 'bestlongitude', 'by']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>hole_id</th>\n",
       "      <th>depth_ft</th>\n",
       "      <th>date_samples</th>\n",
       "      <th>file_name</th>\n",
       "      <th>internal_id</th>\n",
       "      <th>notes</th>\n",
       "      <th>source_samples</th>\n",
       "      <th>user</th>\n",
       "      <th>alfemg</th>\n",
       "      <th>...</th>\n",
       "      <th>starrating1</th>\n",
       "      <th>starrating2</th>\n",
       "      <th>starrating3</th>\n",
       "      <th>starrating4</th>\n",
       "      <th>starrating5</th>\n",
       "      <th>starrating6</th>\n",
       "      <th>userelevation</th>\n",
       "      <th>userlatitude</th>\n",
       "      <th>userlongitude</th>\n",
       "      <th>version</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>T27-020</td>\n",
       "      <td>KM-21-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44761.0</td>\n",
       "      <td>30184_AMC_KayMine_HaloStandard-KM-21-27-S_3018...</td>\n",
       "      <td>0</td>\n",
       "      <td>QAQC blank</td>\n",
       "      <td>KM-21-27.xlsx</td>\n",
       "      <td>AA</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not Set</td>\n",
       "      <td>Not Set</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>T27-040</td>\n",
       "      <td>KM-21-27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44761.0</td>\n",
       "      <td>30184_AMC_KayMine_HaloStandard-KM-21-27-S_3018...</td>\n",
       "      <td>0</td>\n",
       "      <td>QAQC blank</td>\n",
       "      <td>KM-21-27.xlsx</td>\n",
       "      <td>AA</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Not Set</td>\n",
       "      <td>Not Set</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id   hole_id  depth_ft  date_samples  \\\n",
       "0   T27-020  KM-21-27       0.0       44761.0   \n",
       "1   T27-040  KM-21-27       0.0       44761.0   \n",
       "\n",
       "                                           file_name internal_id       notes  \\\n",
       "0  30184_AMC_KayMine_HaloStandard-KM-21-27-S_3018...           0  QAQC blank   \n",
       "1  30184_AMC_KayMine_HaloStandard-KM-21-27-S_3018...           0  QAQC blank   \n",
       "\n",
       "  source_samples user alfemg  ... starrating1  starrating2 starrating3  \\\n",
       "0  KM-21-27.xlsx   AA      0  ...         3.0          0.0         0.0   \n",
       "1  KM-21-27.xlsx   AA      0  ...         3.0          0.0         0.0   \n",
       "\n",
       "  starrating4  starrating5  starrating6 userelevation  userlatitude  \\\n",
       "0         0.0          0.0          0.0           0.0       Not Set   \n",
       "1         0.0          0.0          0.0           0.0       Not Set   \n",
       "\n",
       "   userlongitude version  \n",
       "0        Not Set     2.3  \n",
       "1        Not Set     2.3  \n",
       "\n",
       "[2 rows x 48 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "output location:\n",
      "/Users/sebastiancoombs/Library/CloudStorage/GoogleDrive-sebastianbcoombs@gmail.com/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/drill terraspec_merged.csv\n",
      "---------------------------------------------------\n",
      "['drill xrf assays master.csv', 'drill xrf samples master.csv']\n",
      "drill xrf\n",
      "Merging drill xrf samples master  with drill xrf assays master data\n",
      "using columns\n",
      "sample_id ; sample_id\n",
      "first columns before sort\n",
      "['sample_id', 'hole_id', 'depth_ft_samples', 'date_samples', 'internal_id', 'notes', 'source_samples', 'user_samples', 'depth_ft_assays', '11-16-2022_xrf_km-22-95', 'ag_concentration', 'ag_error1s', 'ag_user_factor_offset', 'ag_user_factor_slope', 'al_concentration']\n",
      "first columns after sort\n",
      "['sample_id', 'hole_id', 'depth_ft_samples', 'date_samples', 'internal_id', 'notes', 'source_samples', 'user_samples', 'depth_ft_assays', '11-16-2022_xrf_km-22-95', 'ag_concentration', 'ag_error1s', 'ag_user_factor_offset', 'ag_user_factor_slope', 'al_concentration']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sample_id</th>\n",
       "      <th>hole_id</th>\n",
       "      <th>depth_ft_samples</th>\n",
       "      <th>date_samples</th>\n",
       "      <th>internal_id</th>\n",
       "      <th>notes</th>\n",
       "      <th>source_samples</th>\n",
       "      <th>user_samples</th>\n",
       "      <th>depth_ft_assays</th>\n",
       "      <th>11-16-2022_xrf_km-22-95</th>\n",
       "      <th>...</th>\n",
       "      <th>y_user_factor_offset</th>\n",
       "      <th>y_user_factor_slope</th>\n",
       "      <th>zn_concentration</th>\n",
       "      <th>zn_error1s</th>\n",
       "      <th>zn_user_factor_offset</th>\n",
       "      <th>zn_user_factor_slope</th>\n",
       "      <th>zr_concentration</th>\n",
       "      <th>zr_error1s</th>\n",
       "      <th>zr_user_factor_offset</th>\n",
       "      <th>zr_user_factor_slope</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18-000</td>\n",
       "      <td>KM-21-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44768.0</td>\n",
       "      <td>26-18</td>\n",
       "      <td>CDN-BL-10</td>\n",
       "      <td>KM-21-18.xlsx</td>\n",
       "      <td>AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>122.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>18-020</td>\n",
       "      <td>KM-21-18</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44768.0</td>\n",
       "      <td>26-39</td>\n",
       "      <td>CDN-BL-10</td>\n",
       "      <td>KM-21-18.xlsx</td>\n",
       "      <td>AA</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 187 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  sample_id   hole_id  depth_ft_samples  date_samples internal_id      notes  \\\n",
       "0    18-000  KM-21-18               0.0       44768.0       26-18  CDN-BL-10   \n",
       "1    18-020  KM-21-18               0.0       44768.0       26-39  CDN-BL-10   \n",
       "\n",
       "  source_samples user_samples  depth_ft_assays  11-16-2022_xrf_km-22-95  ...  \\\n",
       "0  KM-21-18.xlsx           AA              NaN                      0.0  ...   \n",
       "1  KM-21-18.xlsx           AA              NaN                      0.0  ...   \n",
       "\n",
       "  y_user_factor_offset  y_user_factor_slope  zn_concentration  zn_error1s  \\\n",
       "0                  0.0                  1.0              29.0         1.0   \n",
       "1                  0.0                  1.0              29.0         1.0   \n",
       "\n",
       "  zn_user_factor_offset  zn_user_factor_slope  zr_concentration  zr_error1s  \\\n",
       "0                   0.0                   1.0             122.0         1.0   \n",
       "1                   0.0                   1.0             130.0         1.0   \n",
       "\n",
       "  zr_user_factor_offset  zr_user_factor_slope  \n",
       "0                   0.0                   1.0  \n",
       "1                   0.0                   1.0  \n",
       "\n",
       "[2 rows x 187 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------\n",
      "output location:\n",
      "/Users/sebastiancoombs/Library/CloudStorage/GoogleDrive-sebastianbcoombs@gmail.com/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/drill xrf_merged.csv\n"
     ]
    }
   ],
   "source": [
    "files=[f for f in files if 'drill' in f]\n",
    "files=[f for f in files if f.endswith('.csv')]\n",
    "files=[f for f in files if 'merged' not in f]\n",
    "files=[f for f in files if 'shift' not in f]\n",
    "\n",
    "samples=[f for f in files if 'samples' in f]\n",
    "\n",
    "keys=sorted([k.split(' ')[1] for k in samples]+['geochemical'])\n",
    "all_samples=[k for k in samples if k.split(' ')[1] =='samples']\n",
    "d_list=[]\n",
    "keys.remove('samples')\n",
    "print(keys,all_samples)\n",
    "for k in keys:\n",
    "    merge_files=[f for f in files if k in f ]\n",
    "    globed=''.join(merge_files)\n",
    "    if 'geochemical' in globed or 'hyp-pkg' in globed:\n",
    "        merge_files+=all_samples\n",
    "    print('---------------------------------------------------')\n",
    "    merg_files=sorted(merge_files)\n",
    "    print(merge_files)\n",
    "    base=merg_files[0].split(' ')\n",
    "    basename=' '.join(base[:2])\n",
    "    print(basename)\n",
    "\n",
    "    merged=pd.DataFrame()\n",
    "    if len(merge_files)<=2:\n",
    "        s_file=merge_files[-1]\n",
    "        s_name=merge_files[-1].split('.')[0]\n",
    "        s_data=pd.read_csv(dir+s_file,low_memory=False)\n",
    "        s_col=s_data.filter(like='sample').columns[0]\n",
    "\n",
    "\n",
    "        d_file=merge_files[0]\n",
    "        d_name=merge_files[0].split('.')[0]\n",
    "        d_data=pd.read_csv(dir+d_file,low_memory=False)\n",
    "        d_col=d_data.filter(like='sample').columns[0]\n",
    "        d_sub=d_name.split(' ')[2]\n",
    "        \n",
    "        if 'terraspec' in d_file:\n",
    "            s_col=s_data.filter(like='file_name').columns[0]\n",
    "\n",
    "        if 'geochemical' in merged:\n",
    "            s_sub=s_name.split(' ')[1]\n",
    "        else:\n",
    "            s_sub=s_name.split(' ')[2]\n",
    "\n",
    "\n",
    "        \n",
    "        print(f'Merging {s_name}  with {d_name} data')\n",
    "        print('using columns')\n",
    "        print(s_col,';',d_col)\n",
    "        #data=pd.concat([s_data.set_index(s_col),d_data.set_index(d_col)], axis=1, join='inner')\n",
    "        data=pd.merge(  \n",
    "                s_data.drop_duplicates(subset=[s_col],keep='first'),\n",
    "                d_data.drop_duplicates(subset=[d_col],keep='first'),\n",
    "        left_on=s_col,\n",
    "        right_on=d_col,\n",
    "        how='inner',\n",
    "        suffixes=['_'+s_sub,'_'+d_sub],\n",
    "        )\n",
    "    \n",
    "    \n",
    "    if len(merge_files)>2:\n",
    "\n",
    "        s_file=merge_files[-1]\n",
    "        s_name=merge_files[-1].split('.')[0]\n",
    "        s_data=pd.read_csv(dir+s_file,low_memory=False)\n",
    "        s_col=s_data.filter(like='sample').columns[0]\n",
    "\n",
    "        d_file=merge_files[0]\n",
    "        d_name=merge_files[0].split('.')[0]\n",
    "        d_data=pd.read_csv(dir+d_file,low_memory=False)\n",
    "        d_col=d_data.filter(like='sample').columns[0]\n",
    "\n",
    "        d_file2=merge_files[1]\n",
    "        d_name2=merge_files[1].split('.')[0]\n",
    "        d_data2=pd.read_csv(dir+d_file2,low_memory=False)\n",
    "        d_col2=d_data2.filter(like='sample').columns[0]\n",
    "\n",
    "        s_sub=s_name.split(' ')[1]\n",
    "        d_sub=d_name.split(' ')[2]\n",
    "        d_sub2=d_name2.split(' ')[2]\n",
    "\n",
    "\n",
    "        print(f'Merging {s_name} samples with {d_name} samples')\n",
    "        print('using columns')\n",
    "        print(s_col,':',d_col)\n",
    "        #data=pd.concat([s_data.set_index(s_col),d_data.set_index(d_col)], axis=1, join='inner')\n",
    "        data=pd.merge(  \n",
    "                s_data.drop_duplicates(subset=[s_col],keep='first'),\n",
    "                d_data.drop_duplicates(subset=[d_col],keep='first'),\n",
    "        left_on=s_col,\n",
    "        right_on=d_col,\n",
    "        how='outer',\n",
    "        suffixes=['_'+s_sub,'_'+d_sub],\n",
    "        )\n",
    "        print(f'Merging {s_name} + {d_name} with {d_name} data')\n",
    "\n",
    "        data=pd.merge(  \n",
    "                        data.drop_duplicates(subset=[s_col],keep='first'),\n",
    "                        d_data2.drop_duplicates(subset=[d_col2],keep='first'),\n",
    "                        left_on=s_col,\n",
    "                        right_on=d_col,\n",
    "                        how='inner',\n",
    "                        suffixes=['_'+s_name,'_'+d_name2[2]],\n",
    "                        )\n",
    "\n",
    "    data=ct.sort_data(data)\n",
    "    data=ct.reorder_columns(data,verbose=True)\n",
    "    data=ct.fix_depths(data)\n",
    "    data=ct.reorder_columns(data)\n",
    "    display(data.head(2))\n",
    "    d_list.append(data)\n",
    "    print('----------------------------')\n",
    "    output=f'{dir}{basename}_merged.csv'\n",
    "    print('output location:')\n",
    "    print(output)\n",
    "    try:\n",
    "        data.to_csv(output,index=False)\n",
    "    except Exception as e:\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['depth_ft_samples', 'depth_ft_assays']\n",
      "{'depth_ft_samples': 'depth_ft', 'depth_ft_assays': 'depth_ft'}\n",
      "Filling []\n",
      "Filling ['depth_ft']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sample_id                False\n",
       "hole_id                  False\n",
       "depth_ft                 False\n",
       "depth_ft                 False\n",
       "date_samples             False\n",
       "                         ...  \n",
       "zn_user_factor_slope      True\n",
       "zr_concentration         False\n",
       "zr_error1s               False\n",
       "zr_user_factor_offset     True\n",
       "zr_user_factor_slope      True\n",
       "Length: 187, dtype: bool"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=d_list[3]\n",
    "\n",
    "\n",
    "def fix_depths(data,like=['_m','_ft'],verbose=True):\n",
    "    depths=[d for d in data.columns if d.lower().startswith('depth')]\n",
    "    print(depths)\n",
    "    depth_map={d:'_'.join(d.split('_')[:-1]) for d in depths if len(d.split('_'))>2 }\n",
    "    print (depth_map)\n",
    "    \n",
    "    data=data.rename(columns=depth_map)\n",
    "\n",
    "    for l in like:\n",
    "        depths=data.filter(like=l,)\n",
    "        cols=[c for c in depths.columns if c.endswith(l)]\n",
    "        cols=list(set(cols))\n",
    "\n",
    "        if verbose:\n",
    "            print('Filling', cols)\n",
    "        data[cols]=data[cols].replace(0,np.nan)\n",
    "        depths=data[cols].copy()\n",
    "\n",
    "        depths=depths.apply(lambda x: x.fillna(x.mean(skipna=True)),axis=1)\n",
    "        data[cols]=depths\n",
    "        # data.drop(data.columns.isduplicated(),axis=1)\n",
    "    return data\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cols' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper_final_merge.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/sebastiancoombs/Documents/GitHub/kay_curate/Scratchpaper_final_merge.ipynb#ch0000037?line=0'>1</a>\u001b[0m data[cols]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mfillna(x\u001b[39m.\u001b[39mmean(skipna\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)),axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cols' is not defined"
     ]
    }
   ],
   "source": [
    "data[cols].apply(lambda x: x.fillna(x.mean(skipna=True)),axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[cols]=data[cols].apply(lambda x: x.fillna(x.mean(skipna=True)),axis=1)\n",
    "data[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/jc/lq6pqgbn3dlfs9v5g5x4jc_80000gn/T/ipykernel_57952/2890912278.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[cols]=data[cols].replace(0,np.nan)\n",
      "/var/folders/jc/lq6pqgbn3dlfs9v5g5x4jc_80000gn/T/ipykernel_57952/2890912278.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data[cols]=data[cols].apply(lambda x: x.fillna(x.mean(skipna=True)),axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>from_m</th>\n",
       "      <th>to_m</th>\n",
       "      <th>depth_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>143.1</td>\n",
       "      <td>143.9</td>\n",
       "      <td>143.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>313.0</td>\n",
       "      <td>314.6</td>\n",
       "      <td>313.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>278.4</td>\n",
       "      <td>279.8</td>\n",
       "      <td>279.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>281.5</td>\n",
       "      <td>282.4</td>\n",
       "      <td>281.95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>269.4</td>\n",
       "      <td>270.4</td>\n",
       "      <td>269.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9503</th>\n",
       "      <td>658.4</td>\n",
       "      <td>658.4</td>\n",
       "      <td>658.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9504</th>\n",
       "      <td>661.4</td>\n",
       "      <td>661.4</td>\n",
       "      <td>661.40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9505</th>\n",
       "      <td>664.5</td>\n",
       "      <td>664.5</td>\n",
       "      <td>664.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9506</th>\n",
       "      <td>667.5</td>\n",
       "      <td>667.5</td>\n",
       "      <td>667.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9507</th>\n",
       "      <td>670.6</td>\n",
       "      <td>670.6</td>\n",
       "      <td>670.60</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9508 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      from_m   to_m  depth_m\n",
       "0      143.1  143.9   143.50\n",
       "1      313.0  314.6   313.80\n",
       "2      278.4  279.8   279.10\n",
       "3      281.5  282.4   281.95\n",
       "4      269.4  270.4   269.90\n",
       "...      ...    ...      ...\n",
       "9503   658.4  658.4   658.40\n",
       "9504   661.4  661.4   661.40\n",
       "9505   664.5  664.5   664.50\n",
       "9506   667.5  667.5   667.50\n",
       "9507   670.6  670.6   670.60\n",
       "\n",
       "[9508 rows x 3 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data=d_list[1].filter(like='_m')\n",
    "cols=data.columns[:3]\n",
    "data[cols]=data[cols].replace(0,np.nan)\n",
    "data[cols]=data[cols].apply(lambda x: x.fillna(x.mean(skipna=True)),axis=1)\n",
    "data[cols]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "depths.columns=['_'.join(c.split('_')[:2]) for c in depths.columns]\n",
    "d_cols=depths.columns\n",
    "depths.columns.duplicated()\n",
    "depths.T.drop_duplicates().T\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths=depths.T.drop_duplicates(keep='first').T\n",
    "df.groupby(\"name\").transform(lambda x: x.fillna(x.sum))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depths=depths.replace(0,np.nan)\n",
    "depths.columns=['_'.join(c.split('_')[:2]) for c in depths.columns]\n",
    "depths=depths.T.drop_duplicates(keep='first').T\n",
    "df.groupby(\"name\").transform(lambda x: x.fillna(\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_for_merge(data):\n",
    "    for col in data.columns:\n",
    "        if 'depth_ft'in col.lower():\n",
    "            try:\n",
    "                data[col]=pd.to_numeric(data[col],errors='coerce')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "            data['from_ft']=data[col]\n",
    "            data['to_ft']=data['from_ft'].shift(-1)-.5\n",
    "            #convert ft to meters\n",
    "            data['from_m']=data['from_ft']* .3281\n",
    "            data['to_m']=data['to_ft'] * .3281\n",
    "            data=data.drop(data.filter(like='Depth').columns,axis=1)\n",
    "        if ('hole' in col.lower())& ('id' in col.lower()):\n",
    "            data[col]=data[col].astype(object)\n",
    "            data.rename(columns={f'{col}':f'hole_id'},inplace=True)\n",
    "        try: \n",
    "            data[col]=data[col].str.strip()\n",
    "        except Exception as e:\n",
    "            pass\n",
    "            #print(f'{col}: {e}')\n",
    "        data.rename(columns={f'{col}':f'{col.strip().lower()}'},inplace=True)\n",
    "        data=data.drop(data.filter(like='unnamed'),axis=1)\n",
    "        if 'from_ft'in col.lower():\n",
    "            try:\n",
    "                drop_index=data[data['from_ft'].isna()].index\n",
    "                data=data.drop(drop_index,axis=0)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "                #print(f'{e}')\n",
    "    if 'hole_id' not in data.columns:\n",
    "        data['hole_id']=np.nan\n",
    "    if 'recvd wt.'  in data.columns:\n",
    "        try:\n",
    "            data['recvd wt.']=pd.to_numeric(data['recvd wt.'],errors='coerce')\n",
    "            data['from_ft']=pd.to_numeric(data['from_ft'],errors='coerce')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            pass\n",
    "    names=data.loc[:,data.columns.duplicated()].columns\n",
    "    if len(names)>0:\n",
    "        print(f'duplcated column: {names} drop or else the nasty merge bug')\n",
    "        data=data.loc[:,~data.columns.duplicated()].copy()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_merge_groups(data):\n",
    "    data=data.fillna(method='ffill')\n",
    "    data=data.fillna(method='bfill')\n",
    "    data=data.drop_duplicates(keep='first')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explode_depths(data):\n",
    "    data[['from_ft','to_ft']]=data[['from_ft','to_ft']].fillna(0)\n",
    "    if 'sample_id' in data.columns:\n",
    "        zipped=zip(data['sample_id'],data['hole_id'], data['from_ft'], data['to_ft'])\n",
    "        depths=pd.DataFrame([(s_id,h_id, y) for s_id,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                    columns=['sample_id','hole_id','true_depth'])\n",
    "        d_frame=depths.merge(data,on=['sample_id','hole_id'])\n",
    "        d_frame=depths.merge(data,on=['sample_id','hole_id'])\n",
    "    else:\n",
    "        data['START']=data['from_ft']\n",
    "        zipped=zip(data['START'],data['hole_id'], data['from_ft'], data['to_ft'])\n",
    "        depths=pd.DataFrame([(s,h_id, y) for s,h_id, start, end in zipped for y in np.arange(start, end,.5)],\n",
    "                        columns=['START','hole_id','true_depth'])\n",
    "        d_frame=depths.merge(data,on=['START','hole_id'])\n",
    "        d_frame=d_frame.drop('START',axis=1)\n",
    "    d_frame=d_frame.drop(d_frame.filter(like='from').columns,axis=1)\n",
    "    d_frame=d_frame.drop(d_frame.filter(like='to').columns,axis=1)\n",
    "    return d_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pull_start_end(data):\n",
    "    depths=data['true_depth']\n",
    "    start=depths.values[0]\n",
    "    end=depths.values[-1]\n",
    "    data['from_ft']=start\n",
    "    data['to_ft']=end\n",
    "    data=data.drop_duplicates(subset=['from_ft','to_ft'],keep='first')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hole_list=[]\n",
    "sample_list=[]\n",
    "sample_data_names=[]\n",
    "hole_data_names=[]\n",
    "files=[file for file in os.listdir(fconfig.output_path) if file.endswith('.csv')]\n",
    "for file in files:\n",
    "    name=file.split('.')[0]\n",
    "    if len(name)==0:\n",
    "        continue\n",
    "    print(f'read {name}')\n",
    "    try:\n",
    "        data=pd.read_csv(fconfig.output_path+file,low_memory=False,on_bad_lines='skip')\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('likely not a csv')\n",
    "        continue\n",
    "    print(f'clean {name}')\n",
    "    data=clean_for_merge(data)\n",
    "\n",
    "    if 'sample_id' in data.columns:\n",
    "        print(f'{name} has sample ids')\n",
    "        sample_list.append(data)\n",
    "        sample_data_names.append(name)\n",
    "    else:\n",
    "        print(f'{name} does not have sample ids')\n",
    "        print(f'explode {name}')\n",
    "        try:\n",
    "            data=explode_depths(data)\n",
    "            hole_list.append(data)\n",
    "            hole_data_names.append(name)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print('must have (depths and hole ids) or (sample_ids) to merge into master')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the data in each group. holes and samples\n",
    "def merge_dflist(data_list,data_names):\n",
    "    big_df=pd.DataFrame()\n",
    "    non_id_cols=['true_depth', 'hole_id']\n",
    "    for i,data in enumerate(data_list):\n",
    "        ## merge all the data with sample_ids\n",
    "        name=data_names[i]\n",
    "        suf=name.split(' ')[0]\n",
    "        print(f\"####### merge frame {i}: {name} suffix= {suf} #######\")\n",
    "        data=data.astype(object)\n",
    "        big_df=big_df.astype(object)\n",
    "        big_cols=list(big_df.columns)\n",
    "        small_cols=list(data.columns)\n",
    "        \n",
    "        dups=sorted([col for col in big_cols if col in small_cols])\n",
    "        cols=config.main_columns\n",
    "        try:\n",
    "            [dups.remove(x) for x in ['file','folder','start_depth','end_depth']if x in data.columns]\n",
    "        except:\n",
    "            pass\n",
    "        if 'sample_id' not in data.columns:\n",
    "            cols=non_id_cols\n",
    "            try:\n",
    "                big_index=list(big_df.hole_id.unique())\n",
    "                small_index=list(data.hole_id.unique())\n",
    "                same_ids= [id for id in big_index if id in small_index]\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                same_ids=[]\n",
    "            dups=non_id_cols\n",
    "        else:\n",
    "            big_index=list(big_df.index)\n",
    "            small_index=list(data.index)\n",
    "            same_ids= [id for id in big_index if id in small_index]\n",
    "        print(f'common columns {dups}')\n",
    "        n_ids=len(same_ids)\n",
    "        if n_ids==0:\n",
    "            print(f'{n_ids} common ids concat')\n",
    "            big_df=pd.concat([big_df,data],axis=0,join='outer')\n",
    "        else:\n",
    "            print(f'{n_ids} common ids merge')\n",
    "            big_df=big_df.merge(data,on=dups,suffixes=[None,'_'+suf],how='outer')\n",
    "\n",
    "        print(f'data shape {big_df.shape}')\n",
    "    \n",
    "    not_dups=[i for i in big_df.columns if i not in cols]\n",
    "    big_final=pd.concat([big_df[cols],big_df[not_dups]],axis=1)\n",
    "\n",
    "    big_final.drop(big_final.filter(like='file').columns,axis=1,inplace=True)\n",
    "    big_final.drop(big_final.filter(like='folder').columns,axis=1,inplace=True)\n",
    "    big_final.sort_index(inplace=True)\n",
    "    return big_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "big_samples=merge_dflist(sample_list,sample_data_names)\n",
    "print('FINISHED MERGING SAMPLES')\n",
    "big_holes=merge_dflist(hole_list,hole_data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=big_samples\n",
    "def list_fill(data,column,id):\n",
    "    data=data[data[column]==id].fillna(method='ffill').fillna(method='bfill').drop_duplicates(keep='first')\n",
    "    return data.copy()\n",
    "def fast_fill_merge(data,column,desc):\n",
    "    dup_id=list(data[data[column].notna().duplicated()][column].unique())\n",
    "    data_list=[list_fill(data,column,id) for id in tqdm(dup_id,desc='Merging on depth')]\n",
    "    print('done listing')\n",
    "    data=pd.concat(data_list,axis=0)\n",
    "    print('done with concat')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_start_end(data,column,id):\n",
    "    data=data[data[column]==id]\n",
    "    depths=data['true_depth']\n",
    "    data=data.drop('true_depth',axis=1)\n",
    "    data=data.drop_duplicates(keep='first')\n",
    "    start=depths.values[0]\n",
    "    end=depths.values[-1]\n",
    "    data['from_ft']=start\n",
    "    data['to_ft']=end\n",
    "    data=data.drop_duplicates(keep='first').copy()\n",
    "    return data\n",
    "    \n",
    "def fast_pull_ends(data,column):\n",
    "    dup_id=list(data[data[column].notna().duplicated()][column].unique())\n",
    "    data_list=[list_start_end(data,column,id) for id in tqdm(dup_id,desc='pulling from/too depths:')]\n",
    "    print('done listing data with ends')\n",
    "    data=pd.concat(data_list,axis=0)\n",
    "    print('done with concat')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#####clean the sample_ids#########')\n",
    "big_samples=fast_fill_merge(data,column='sample_id',desc='clean_samples')\n",
    "big_samples\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('#####explode sample_ids#########')\n",
    "samp_ex=explode_depths(big_samples)\n",
    "print('######### merge holes and samples ##########')\n",
    "merged=samp_ex.merge(big_holes,on=['hole_id','true_depth'],how='outer')\n",
    "print('######### fill the final_data ##########')\n",
    "merged['u_id']=merged.fillna(np.inf).groupby(['hole_id','true_depth']).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test=fast_fill_merge(merged,column='u_id',desc='clean total_data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test['u_id']=merged_test.fillna(np.inf).drop('true_depth',axis=1).groupby(merged_test.drop('true_depth',axis=1).columns.to_list()).ngroup()\n",
    "len(merged_test['u_id'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test=fast_pull_ends(merged_test,column='u_id')\n",
    "final_cols=['sample_id','hole_id','from_ft','to_ft']\n",
    "des_cols = merged_test.filter(like='description').columns\n",
    "other_cols= [col for col in merged_test.columns if col not in final_cols]\n",
    "merged_test=pd.concat([merged_test[final_cols],merged_test[des_cols],merged_test[other_cols]],axis=1)\n",
    "merged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['u_id']=merged.fillna(np.inf).groupby(merged.drop('true_depth',axis=1).columns.to_list()).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "merged_test=fast_pull_ends(merged,column='u_id',desc='pull start ends')\n",
    "final_cols=['sample_id','hole_id','from_ft','to_ft']\n",
    "des_cols = merged_test.filter(like='description').columns\n",
    "other_cols= [col for col in merged_test.columns if col not in final_cols]\n",
    "merged_test=pd.concat([merged_test[final_cols],merged_test[des_cols],merged_test[other_cols]],axis=1)\n",
    "merged_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['u_id']=merged.fillna(np.inf).groupby(merged.drop('true_depth',axis=1).columns.to_list()).ngroup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged['from_ft']=np.inf\n",
    "merged['to_ft']=np.inf\n",
    "merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test=merged.groupby(merged['u_id']).progress_apply(pull_start_end).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.drop('true_depth',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_cols=['sample_id','hole_id','from_ft','to_ft']\n",
    "des_cols = merged_test.filter(like='description').columns\n",
    "other_cols= [col for col in merged_test.columns if col not in final_cols]\n",
    "merged_test=pd.concat([merged_test[final_cols],merged_test[des_cols],merged_test[other_cols]],axis=1)\n",
    "merged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test=merged_test.T.drop_duplicates(keep='first').T\n",
    "merged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for col in merged_test.columns:\n",
    "    if merged_test[col].isna().sum()==len(merged_test):\n",
    "        print(f'drop {col}: no data')\n",
    "        merged_test=merged_test.drop(col,axis=1)\n",
    "merged_test=merged_test.sort_values(by=['sample_id','hole_id','from_ft'],ascending=False)\n",
    "merged_test=merged_test.set_index('sample_id')\n",
    "merged_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_test.to_excel('/Volumes/GoogleDrive/Shared drives/AMC Projects/_AZ_Kay/_Master Databases/master_MASTER.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
